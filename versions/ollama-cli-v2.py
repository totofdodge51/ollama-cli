#!/usr/bin/env python3
"""
Ollama CLI v2 - Assistant LLM interactif am√©lior√© avec des capacit√©s avanc√©es de gestion de fichiers.
"""

import os
import sys
import json
import argparse
import requests
from pathlib import Path
from typing import List, Optional, Tuple
import readline
import atexit
import subprocess
import difflib
import xml.etree.ElementTree as ET
import re

# Importations de la biblioth√®que Rich pour une interface utilisateur riche
from rich.console import Console
from rich.markdown import Markdown
from rich.syntax import Syntax
from rich.panel import Panel
from rich.prompt import Prompt, Confirm
from rich.table import Table
from rich.live import Live
from rich.spinner import Spinner

# Initialisation de la console Rich pour un affichage esth√©tique
console = Console()

# Fichiers pour l'historique et le contexte
HISTORY_FILE = Path.home() / ".ollama_cli_history"
CONTEXT_FILE = Path.home() / ".ollama_cli_context.json"

class OllamaAPI:
    """Wrapper pour les appels √† l'API d'Ollama"""

    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.model = "llama3"  # Mod√®le par d√©faut
        # --- AM√âLIORATION : Prompt Syst√®me plus robuste ---
        # Ce nouveau prompt guide l'IA pour cr√©er ET modifier des fichiers
        # en utilisant un format XML structur√©, ce qui fiabilise le parsing.
        self.system_prompt_template = """Vous √™tes un assistant de d√©veloppement expert dans un terminal de ligne de commande.

Votre but est d'aider l'utilisateur avec son code. Les fichiers suivants sont actuellement charg√©s en contexte : {loaded_files}.

- Pour une conversation g√©n√©rale, r√©pondez de mani√®re claire et concise.
- Pour **cr√©er un nouveau fichier**, utilisez le format <plan> et <code>.
- Pour **modifier un ou plusieurs fichiers existants**, r√©pondez en utilisant **uniquement** le format XML suivant. Ne mettez AUCUN texte avant ou apr√®s le bloc <file_modifications>.

<file_modifications>
  <file path="chemin/vers/fichier1.py">
  <![CDATA[
Le contenu INTEGRAL et MODIFI√â du fichier1.py va ici.
]]>
  </file>
  <file path="chemin/vers/fichier2.js">
  <![CDATA[
Le contenu INTEGRAL et MODIFI√â du fichier2.js va ici.
]]>
  </file>
</file_modifications>

- Fournissez toujours le contenu complet du fichier, pas seulement un extrait ou un diff.
- L'utilisateur verra un diff des changements et devra confirmer avant d'enregistrer."""

    def get_system_prompt(self, loaded_files: List[str]) -> str:
        """Formate le prompt syst√®me avec les fichiers actuellement charg√©s."""
        files_str = ", ".join(loaded_files) if loaded_files else "aucun"
        return self.system_prompt_template.format(loaded_files=files_str)

    def list_models(self) -> List[str]:
        """Liste les mod√®les Ollama disponibles."""
        try:
            response = requests.get(f"{self.base_url}/api/tags")
            response.raise_for_status()
            models = response.json().get("models", [])
            return [model["name"] for model in models]
        except requests.exceptions.RequestException as e:
            console.print(f"[red]Erreur de connexion √† l'API Ollama : {e}[/red]")
            return []

    def generate(self, prompt: str, system_prompt: str, context: Optional[List] = None, stream: bool = True) -> str:
        """G√©n√®re une r√©ponse depuis Ollama, en stream par d√©faut."""
        payload = {
            "model": self.model,
            "prompt": prompt,
            "system": system_prompt,
            "stream": stream,
            "context": context or []
        }

        try:
            response = requests.post(f"{self.base_url}/api/generate", json=payload, stream=stream)
            response.raise_for_status()

            full_response = ""
            if stream:
                # --- AM√âLIORATION : Affichage en streaming avec Live et Spinner ---
                spinner = Spinner("dots", " L'assistant r√©fl√©chit...")
                with Live(spinner, refresh_per_second=10, transient=True) as live:
                    for line in response.iter_lines():
                        if line:
                            data = json.loads(line)
                            chunk = data.get("response", "")
                            full_response += chunk
                            # Mettre √† jour le Live avec le texte g√©n√©r√©
                            live.update(Markdown(full_response), refresh=True)

                            if data.get("done", False):
                                self.last_context = data.get("context", [])
                                break
                console.print(Markdown(full_response))
                return full_response
            else:
                # Mode non-stream (pourrait √™tre utilis√© pour des t√¢ches de fond)
                data = response.json()
                full_response = data.get("response", "")
                self.last_context = data.get("context", [])
                return full_response

        except requests.exceptions.RequestException as e:
            console.print(f"[red]Erreur lors de l'appel √† l'API Ollama : {e}[/red]")
            return ""
        except json.JSONDecodeError:
            console.print("[red]Erreur: R√©ponse invalide de l'API Ollama.[/red]")
            return ""

class FileHandler:
    """G√®re les op√©rations sur les fichiers et l'affichage des modifications."""

    @staticmethod
    def read_file(filepath: Path) -> Tuple[bool, str]:
        """Lit le contenu d'un fichier."""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                return True, f.read()
        except Exception as e:
            return False, str(e)

    @staticmethod
    def write_file(filepath: Path, content: str) -> Tuple[bool, str]:
        """√âcrit du contenu dans un fichier."""
        try:
            filepath.parent.mkdir(parents=True, exist_ok=True)
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            return True, f"Fichier sauvegard√© : {filepath}"
        except Exception as e:
            return False, str(e)

    @staticmethod
    def show_diff(original: str, modified: str, filepath: str) -> None:
        """Affiche un diff color√© entre deux versions de contenu."""
        diff = difflib.unified_diff(
            original.splitlines(keepends=True),
            modified.splitlines(keepends=True),
            fromfile=f"{filepath} (original)",
            tofile=f"{filepath} (modifi√©)"
        )

        console.print(Panel(f"Changements propos√©s pour [bold cyan]{filepath}[/bold cyan]", border_style="yellow"))
        diff_lines = list(diff)
        if not diff_lines:
            console.print("[dim]Aucun changement d√©tect√©.[/dim]")
            return

        for line in diff_lines:
            if line.startswith('+'):
                console.print(f"[green]{line}[/green]", end="")
            elif line.startswith('-'):
                console.print(f"[red]{line}[/red]", end="")
            elif line.startswith('@@'):
                console.print(f"[yellow]{line}[/yellow]", end="")
            else:
                console.print(line, end="")

class OllamaCLI:
    """Application CLI principale."""

    def __init__(self):
        self.api = OllamaAPI()
        self.file_handler = FileHandler()
        self.context = []
        self.conversation_history = []
        self.working_directory = Path.cwd()
        self.loaded_files = {}  # Dict pour stocker le contenu des fichiers charg√©s
        self.setup_readline()

    def setup_readline(self):
        """Configure readline pour l'historique des commandes."""
        if HISTORY_FILE.exists():
            readline.read_history_file(HISTORY_FILE)
        readline.set_history_length(1000)
        atexit.register(readline.write_history_file, HISTORY_FILE)

    def load_context(self):
        """Charge le contexte de la conversation pr√©c√©dente depuis un fichier."""
        if CONTEXT_FILE.exists():
            try:
                with open(CONTEXT_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.context = data.get("context", [])
                    self.conversation_history = data.get("history", [])
                    console.print("[dim]Contexte de la conversation pr√©c√©dente charg√©.[/dim]")
            except (json.JSONDecodeError, IOError) as e:
                console.print(f"[yellow]Avertissement : Impossible de charger le contexte : {e}[/yellow]")
                self.context = []
                self.conversation_history = []

    def save_context(self):
        """Sauvegarde le contexte de la conversation actuelle dans un fichier."""
        try:
            with open(CONTEXT_FILE, 'w', encoding='utf-8') as f:
                context_to_save = {
                    "context": self.context,
                    "history": self.conversation_history[-20:]  # Garder les 20 derniers √©changes
                }
                json.dump(context_to_save, f, indent=2)
        except IOError as e:
            console.print(f"[yellow]Avertissement : Impossible de sauvegarder le contexte : {e}[/yellow]")

    def select_model(self) -> bool:
        """Permet √† l'utilisateur de s√©lectionner un mod√®le Ollama."""
        console.print(Panel("ü§ñ Mod√®les Disponibles", border_style="blue"))
        models = self.api.list_models()
        if not models:
            console.print("[red]Aucun mod√®le Ollama trouv√©. Assurez-vous qu'Ollama est en cours d'ex√©cution et que des mod√®les sont install√©s (ex: `ollama pull llama3`).[/red]")
            return False

        table = Table(show_header=True, header_style="bold magenta")
        table.add_column("Index", style="cyan")
        table.add_column("Nom du Mod√®le", style="green")

        for i, model in enumerate(models, 1):
            table.add_row(str(i), model)

        console.print(table)

        choices = [str(i) for i in range(1, len(models) + 1)]
        choice = Prompt.ask("S√©lectionnez un mod√®le", choices=choices, default="1")

        idx = int(choice) - 1
        self.api.model = models[idx]
        console.print(f"[green]‚úì Mod√®le s√©lectionn√© : [bold]{self.api.model}[/bold][/green]")
        return True

    def handle_command(self, command: str) -> Tuple[bool, Optional[str]]:
        """G√®re les commandes sp√©ciales commen√ßant par '/'.

        Returns:
            Tuple[bool, Optional[str]]: (continue_loop, processed_input)
        """
        parts = command.split()
        cmd = parts[0].lower()

        # --- AM√âLIORATION : Remplacement du dictionnaire par des if/elif pour plus de flexibilit√© ---
        if cmd in ["/quit", "/exit", "/q"]:
            console.print("Au revoir !")
            return False, None

        elif cmd == "/help":
            self.show_help()
            return True, None

        elif cmd == "/clear":
            self.clear_context()
            console.print("[green]Contexte effac√©.[/green]")
            return True, None

        elif cmd == "/model":
            self.select_model()
            return True, None

        elif cmd == "/load":
            if len(parts) > 1:
                self.load_file(Path(" ".join(parts[1:])))
            else:
                console.print("[red]Usage: /load <filepath>[/red]")
            return True, None

        elif cmd == "/edit":
            if len(parts) > 1:
                self.edit_file(Path(" ".join(parts[1:])))
            else:
                console.print("[red]Usage: /edit <filepath>[/red]")
            return True, None

        # --- NOUVEAUT√â : Commande /paste pour la saisie multiligne ---
        elif cmd == "/paste":
            pasted_text = self.get_pasted_input()
            # On retourne le texte coll√© pour qu'il soit trait√© par la boucle de chat
            return True, pasted_text

        elif cmd == "/run":
            if len(parts) > 1:
                self.run_command(" ".join(parts[1:]))
            else:
                console.print("[red]Usage: /run <command>[/red]")
            return True, None

        elif cmd == "/files":
            self.list_loaded_files()
            return True, None

        elif cmd == "/pwd":
            console.print(f"[cyan]R√©pertoire actuel : {self.working_directory}[/cyan]")
            return True, None

        elif cmd == "/cd":
            if len(parts) > 1:
                self.change_directory(Path(" ".join(parts[1:])))
            else:
                console.print("[red]Usage: /cd <directory>[/red]")
            return True, None

        else:
            console.print(f"[red]Commande inconnue : {cmd}. Tapez /help pour la liste.[/red]")
            return True, None

    def clear_context(self):
        self.context = []
        self.conversation_history = []
        self.loaded_files = {}

    def show_help(self):
        help_text = """
#  Aide de Ollama CLI v2

## Commandes de Chat
- **`/quit`, `/exit`, `/q`**: Quitter l'application.
- **`/clear`**: Effacer l'historique de la conversation et les fichiers charg√©s.
- **`/model`**: S√©lectionner un autre mod√®le Ollama.

## Commandes de Fichiers
- **`/load <fichier>`**: Charger un fichier dans le contexte.
- **`/paste`**: Activer le mode collage pour envoyer du code ou du texte multiligne.
- **`/edit <fichier>`**: Lancer une session d'√©dition assist√©e par l'IA sur un fichier.
- **`/files`**: Lister les fichiers actuellement charg√©s en m√©moire.
- **`/pwd`**: Afficher le r√©pertoire de travail actuel.
- **`/cd <dossier>`**: Changer de r√©pertoire de travail.

## Commandes Syst√®me
- **`/run <commande>`**: Ex√©cuter une commande shell.
- **`/help`**: Afficher ce message d'aide.
        """
        console.print(Panel(Markdown(help_text), title=" Aide ", border_style="green"))

    def load_file(self, filepath: Path):
        full_path = self.working_directory / filepath
        success, content = self.file_handler.read_file(full_path)

        if success:
            self.loaded_files[str(filepath)] = content
            lexer = self.guess_lexer(filepath)
            preview = Syntax(content[:1000] + ("..." if len(content) > 1000 else ""), lexer, theme="monokai", line_numbers=True)
            console.print(Panel(preview, title=f"üìÇ Fichier charg√© : {filepath}", subtitle=f"{len(content)} caract√®res", border_style="blue"))
        else:
            console.print(f"[red]Erreur de chargement : {content}[/red]")

    def edit_file(self, filepath: Path):
        """Session d'√©dition interactive d'un fichier."""
        full_path = self.working_directory / filepath
        success, original_content = self.file_handler.read_file(full_path)

        if not success:
            console.print(f"[red]Impossible de lire le fichier : {original_content}[/red]")
            return

        console.print(f"[cyan]√âdition de : {filepath}[/cyan]")
        edit_request = Prompt.ask("[bold]D√©crivez les changements souhait√©s[/bold]")

        prompt = f"""L'utilisateur souhaite modifier le fichier '{filepath}'.
Voici le contenu original :
```
{original_content}
```
Voici la demande de l'utilisateur : "{edit_request}".

Veuillez fournir la r√©ponse en utilisant le format <file_modifications> avec le contenu complet et mis √† jour du fichier."""

        system_prompt = self.api.get_system_prompt(list(self.loaded_files.keys()))
        response = self.api.generate(prompt, system_prompt, self.context)
        self.process_response(response)

    def run_command(self, command: str):
        console.print(f"[bold cyan]Ex√©cution : $ {command}[/bold cyan]")
        try:
            with subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, cwd=self.working_directory) as proc:
                if proc.stdout:
                    for line in proc.stdout:
                        console.print(f"[dim]{line.strip()}[/dim]")
                if proc.stderr:
                    for line in proc.stderr:
                        console.print(f"[red]{line.strip()}[/red]")
        except Exception as e:
            console.print(f"[red]Erreur d'ex√©cution : {e}[/red]")

    def list_loaded_files(self):
        if not self.loaded_files:
            console.print("[dim]Aucun fichier charg√©.[/dim]")
            return

        table = Table(title=" ‡§´‡§æ‡§á‡§≤‡•ç‡§∏ en Contexte", border_style="purple")
        table.add_column("Chemin", style="cyan")
        table.add_column("Taille (caract√®res)", style="green")

        for filepath, content in self.loaded_files.items():
            table.add_row(filepath, str(len(content)))
        console.print(table)

    def change_directory(self, new_dir: Path):
        """Change le r√©pertoire de travail."""
        try:
            target_path = (self.working_directory / new_dir).resolve()
            if target_path.is_dir():
                os.chdir(target_path)
                self.working_directory = target_path
                console.print(f"[green]Nouveau r√©pertoire : {self.working_directory}[/green]")
            else:
                console.print(f"[red]R√©pertoire non trouv√© : {target_path}[/red]")
        except Exception as e:
            console.print(f"[red]Erreur : {e}[/red]")


    def guess_lexer(self, filepath: Path, code: str = "") -> str:
        # Utilise le nom de fichier d'abord, puis le contenu si pas de nom
        return Syntax.guess_lexer(str(filepath), code=code)

    # --- NOUVEAUT√â : M√©thode pour g√©rer la saisie multiligne ---
    def get_pasted_input(self) -> str:
        """R√©cup√®re une saisie multiligne de l'utilisateur."""
        console.print("[dim]Collez votre texte. Appuyez sur [bold]Ctrl+D[/bold] (ou Ctrl+Z sous Windows) pour terminer.[/dim]")
        try:
            pasted_text = sys.stdin.read().strip()
            if pasted_text:
                # Devine le langage pour une jolie coloration syntaxique
                lexer = self.guess_lexer(Path("pasted.txt"), code=pasted_text)
                console.print(Panel(
                    Syntax(pasted_text, lexer, theme="monokai", line_numbers=True),
                    title="Texte coll√©",
                    border_style="green"
                ))
            else:
                console.print("[yellow]Aucune saisie d√©tect√©e.[/yellow]")
            return pasted_text
        except Exception as e:
            console.print(f"[red]Erreur lors de la lecture de la saisie : {e}[/red]")
            return ""


    # --- AM√âLIORATION : Processeur de r√©ponse intelligent ---
    # Cette fonction est au c≈ìur de la nouvelle logique. Elle d√©tecte si l'IA
    # veut cr√©er, modifier ou simplement discuter.
    def process_response(self, response: str):
        """Analyse la r√©ponse de l'IA pour des actions sp√©ciales (cr√©ation/modification de fichiers)."""
        try:
            if '<file_modifications>' in response:
                self.handle_file_modifications(response)
            elif '<plan>' in response and '<code>' in response:
                self.handle_file_creation(response)
            else:
                # Si aucune action sp√©ciale n'est d√©tect√©e, on affiche la r√©ponse telle quelle.
                # La r√©ponse est d√©j√† affich√©e par le stream, cette partie peut √™tre vide.
                pass
        except Exception as e:
            console.print(f"[bold red]Erreur lors de l'analyse de la r√©ponse de l'IA :[/bold red] {e}")
            console.print("[dim]Affichage de la r√©ponse brute :[/dim]")
            console.print(response)

    def handle_file_modifications(self, response: str):
        """G√®re la logique de modification de fichiers propos√©e par l'IA."""
        console.print("[bold yellow]L'assistant propose des modifications de fichiers...[/bold yellow]")
        # Envelopper dans une balise racine pour un parsing XML valide
        xml_response = f"<root>{response}</root>"
        root = ET.fromstring(xml_response)

        modifications = []
        for file_elem in root.findall('.//file'):
            filepath_str = file_elem.get('path')
            content = (file_elem.text or "").strip()
            if filepath_str and content:
                modifications.append({'path': Path(filepath_str), 'content': content})

        if not modifications:
            console.print("[red]L'IA a tent√© de modifier un fichier mais le format √©tait incorrect.[/red]")
            return

        for mod in modifications:
            full_path = self.working_directory / mod['path']
            original_content = ""
            if full_path.exists():
                _, original_content = self.file_handler.read_file(full_path)
            self.file_handler.show_diff(original_content, mod['content'], str(mod['path']))

        if Confirm.ask("\n[bold]Appliquer ces modifications ?[/bold]", default=True):
            for mod in modifications:
                success, msg = self.file_handler.write_file(self.working_directory / mod['path'], mod['content'])
                if success:
                    console.print(f"[green]‚úì {msg}[/green]")
                    self.loaded_files[str(mod['path'])] = mod['content'] # Mettre √† jour le contexte
                else:
                    console.print(f"[red]‚úó Erreur sur {mod['path']}: {msg}[/red]")
        else:
            console.print("[yellow]Modifications annul√©es.[/yellow]")


    def handle_file_creation(self, response: str):
        """G√®re la logique de cr√©ation de fichier."""
        plan_match = re.search(r'<plan>(.*?)</plan>', response, re.DOTALL)
        code_match = re.search(r'<code>(.*?)</code>', response, re.DOTALL)

        if not (plan_match and code_match): return

        plan = plan_match.group(1).strip()
        code = code_match.group(1).strip()

        console.print(Panel(Markdown(plan), title="Plan de Cr√©ation Propos√©", border_style="blue"))

        if Confirm.ask("\n[bold]Proc√©der √† la cr√©ation de ce fichier ?[/bold]"):
            filename_match = re.search(r'filename:\s*(\S+)', plan, re.IGNORECASE)
            default_filename = filename_match.group(1) if filename_match else "nouveau_fichier.py"

            filename_str = Prompt.ask("Nom du fichier", default=default_filename)
            filepath = self.working_directory / filename_str

            success, msg = self.file_handler.write_file(filepath, code)
            if success:
                console.print(f"[green]‚úì {msg}[/green]")
                if Confirm.ask("Charger ce nouveau fichier en contexte ?", default=True):
                    self.load_file(Path(filename_str))
            else:
                console.print(f"[red]‚úó {msg}[/red]")
        else:
            console.print("[yellow]Cr√©ation annul√©e.[/yellow]")

    def chat_loop(self):
        """Boucle principale de chat."""
        console.print(Panel(
            "[bold cyan]Bienvenue dans Ollama CLI v2[/bold cyan]\n"
            "[dim]Votre assistant de code interactif. Tapez [yellow]/help[/yellow] pour les commandes.[/dim]",
            border_style="cyan",
            expand=False
        ))

        if not self.select_model():
            return

        self.load_context()

        while True:
            try:
                prompt = self.build_full_prompt()
                user_input = console.input(prompt)

                if not user_input.strip():
                    continue

                # --- AM√âLIORATION : Logique de boucle de chat pour g√©rer /paste ---
                if user_input.startswith('/'):
                    continue_loop, processed_input = self.handle_command(user_input)

                    if not continue_loop:
                        self.save_context()
                        break  # Quitte la boucle principale

                    if processed_input is not None:
                        # La commande /paste a retourn√© du texte, on l'utilise comme saisie
                        user_input = processed_input
                        if not user_input: # Si le collage est vide, on reboucle
                            continue
                    else:
                        # C'√©tait une autre commande (ex: /help), on ne l'envoie pas √† l'IA
                        continue

                self.conversation_history.append(("user", user_input))

                # Le prompt complet envoy√© √† l'IA inclut le contenu des fichiers
                full_content_prompt = self.get_files_content_for_prompt() + user_input
                system_prompt = self.api.get_system_prompt(list(self.loaded_files.keys()))

                response = self.api.generate(full_content_prompt, system_prompt, self.context)

                # Le post-traitement se fait apr√®s avoir re√ßu la r√©ponse compl√®te
                self.process_response(response)

                self.conversation_history.append(("assistant", response))
                self.save_context()

            except (KeyboardInterrupt, EOFError):
                self.save_context()
                console.print("\n[yellow]Au revoir ![/yellow]")
                break

    def build_full_prompt(self) -> str:
        """Construit le prompt affich√© √† l'utilisateur, incluant le contexte."""
        # Pourrait √™tre √©tendu pour afficher le statut (ex: fichiers charg√©s)
        return "\n[bold green]Vous[/bold green] > "

    def get_files_content_for_prompt(self) -> str:
        """Concat√®ne le contenu des fichiers charg√©s pour l'injecter dans le prompt."""
        if not self.loaded_files:
            return ""

        context_str = "CONTEXTE :\n"
        for path, content in self.loaded_files.items():
            context_str += f"--- Contenu de {path} ---\n{content}\n--- Fin de {path} ---\n\n"
        return context_str


def main():
    parser = argparse.ArgumentParser(description="Ollama CLI v2 - Assistant de code interactif.")
    parser.add_argument("--api-url", default="http://localhost:11434", help="URL de l'API Ollama.")
    parser.add_argument("--model", help="Sp√©cifier un mod√®le √† utiliser directement.")
    parser.add_argument("files", nargs="*", help="Fichiers √† charger au d√©marrage.")
    args = parser.parse_args()

    cli = OllamaCLI()
    cli.api.base_url = args.api_url
    if args.model:
        cli.api.model = args.model

    # Charger les fichiers pass√©s en argument
    if args.files:
        for f in args.files:
            cli.load_file(Path(f))

    cli.chat_loop()

if __name__ == "__main__":
    main()





